{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:39:44.372136Z","iopub.execute_input":"2025-02-17T02:39:44.372416Z","iopub.status.idle":"2025-02-17T02:39:47.765249Z","shell.execute_reply.started":"2025-02-17T02:39:44.372394Z","shell.execute_reply":"2025-02-17T02:39:47.763942Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import google.generativeai as genai\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:39:47.766700Z","iopub.execute_input":"2025-02-17T02:39:47.767028Z","iopub.status.idle":"2025-02-17T02:39:47.771475Z","shell.execute_reply.started":"2025-02-17T02:39:47.766998Z","shell.execute_reply":"2025-02-17T02:39:47.770323Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:42:30.100150Z","iopub.execute_input":"2025-02-17T02:42:30.100486Z","iopub.status.idle":"2025-02-17T02:42:30.418562Z","shell.execute_reply.started":"2025-02-17T02:42:30.100460Z","shell.execute_reply":"2025-02-17T02:42:30.417486Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import google.generativeai as genai\n\ngenai.configure(api_key=\"AIzaSyBUtIhUVHs8xLDLKOGoN6qUAmhSIolxItc\")  # Reemplaza con tu clave API\n\nflash = genai.GenerativeModel('gemini-1.5-flash')\nresponse = flash.generate_content(\"Explain AI to me like I'm a kid.\")\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:45:34.586935Z","iopub.execute_input":"2025-02-17T02:45:34.587255Z","iopub.status.idle":"2025-02-17T02:45:36.772628Z","shell.execute_reply.started":"2025-02-17T02:45:34.587228Z","shell.execute_reply":"2025-02-17T02:45:36.771211Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a really smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  The more you teach it, the better it gets at those tricks.\n\nAI is kind of like that super smart puppy, but instead of tricks, it learns from information.  We give it lots and lots of information – like pictures of cats and dogs, or words from books.  The AI learns to recognize patterns in that information, just like your puppy learns to recognize the sound of your voice saying \"fetch.\"\n\nSo, an AI that learned about cats and dogs could look at a new picture and tell you if it's a cat or a dog, even if it's never seen that exact picture before!  It's figuring things out on its own, based on what it has learned.\n\nSome AIs are good at playing games, some are good at writing stories, and some are good at helping doctors make diagnoses.  They're all different, just like different puppies have different talents!  But they all learn from information to become smarter.\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:45:59.591432Z","iopub.execute_input":"2025-02-17T02:45:59.591741Z","iopub.status.idle":"2025-02-17T02:45:59.601300Z","shell.execute_reply.started":"2025-02-17T02:45:59.591709Z","shell.execute_reply":"2025-02-17T02:45:59.599102Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine you have a really smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  The more you teach it, the better it gets at those tricks.\n\nAI is kind of like that super smart puppy, but instead of tricks, it learns from information.  We give it lots and lots of information – like pictures of cats and dogs, or words from books.  The AI learns to recognize patterns in that information, just like your puppy learns to recognize the sound of your voice saying \"fetch.\"\n\nSo, an AI that learned about cats and dogs could look at a new picture and tell you if it's a cat or a dog, even if it's never seen that exact picture before!  It's figuring things out on its own, based on what it has learned.\n\nSome AIs are good at playing games, some are good at writing stories, and some are good at helping doctors make diagnoses.  They're all different, just like different puppies have different talents!  But they all learn from information to become smarter.\n"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"chat = flash.start_chat(history=[])\nresponse = chat.send_message('Hello! My name is Manuel.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:46:19.580465Z","iopub.execute_input":"2025-02-17T02:46:19.580779Z","iopub.status.idle":"2025-02-17T02:46:20.139556Z","shell.execute_reply.started":"2025-02-17T02:46:19.580753Z","shell.execute_reply":"2025-02-17T02:46:20.138422Z"}},"outputs":[{"name":"stdout","text":"Hello Manuel! It's nice to meet you.  How can I help you today?\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"response = chat.send_message('Can you tell something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:46:33.578567Z","iopub.execute_input":"2025-02-17T02:46:33.578916Z","iopub.status.idle":"2025-02-17T02:46:34.626134Z","shell.execute_reply.started":"2025-02-17T02:46:33.578861Z","shell.execute_reply":"2025-02-17T02:46:34.625068Z"}},"outputs":[{"name":"stdout","text":"Did you know that some dinosaurs had feathers?  While we often picture dinosaurs as scaly reptiles,  many species, particularly theropods (the group that includes *Tyrannosaurus rex* and *Velociraptor*), are now known to have had feathers, ranging from simple filaments to complex, bird-like plumage. This discovery significantly changed our understanding of dinosaur evolution and their relationship to birds.  It's fascinating evidence supporting the theory that birds evolved from feathered dinosaurs.\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\nresponse = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:46:50.735176Z","iopub.execute_input":"2025-02-17T02:46:50.735473Z","iopub.status.idle":"2025-02-17T02:46:51.212937Z","shell.execute_reply.started":"2025-02-17T02:46:50.735447Z","shell.execute_reply":"2025-02-17T02:46:51.211979Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Manuel.\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"for model in genai.list_models():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:47:03.412440Z","iopub.execute_input":"2025-02-17T02:47:03.412766Z","iopub.status.idle":"2025-02-17T02:47:03.700398Z","shell.execute_reply.started":"2025-02-17T02:47:03.412729Z","shell.execute_reply":"2025-02-17T02:47:03.699487Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/gemini-2.0-flash-exp\nmodels/gemini-2.0-flash\nmodels/gemini-2.0-flash-001\nmodels/gemini-2.0-flash-lite-preview\nmodels/gemini-2.0-flash-lite-preview-02-05\nmodels/gemini-2.0-pro-exp\nmodels/gemini-2.0-pro-exp-02-05\nmodels/gemini-exp-1206\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/learnlm-1.5-pro-experimental\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\nmodels/imagen-3.0-generate-002\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"for model in genai.list_models():\n  if model.name == 'models/gemini-1.5-flash':\n    print(model)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:47:15.380995Z","iopub.execute_input":"2025-02-17T02:47:15.381286Z","iopub.status.idle":"2025-02-17T02:47:15.654309Z","shell.execute_reply.started":"2025-02-17T02:47:15.381266Z","shell.execute_reply":"2025-02-17T02:47:15.653163Z"}},"outputs":[{"name":"stdout","text":"Model(name='models/gemini-1.5-flash',\n      base_model_id='',\n      version='001',\n      display_name='Gemini 1.5 Flash',\n      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n      input_token_limit=1000000,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=40)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"short_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(max_output_tokens=200))\n\nresponse = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:47:26.910485Z","iopub.execute_input":"2025-02-17T02:47:26.910782Z","iopub.status.idle":"2025-02-17T02:47:28.550392Z","shell.execute_reply.started":"2025-02-17T02:47:26.910746Z","shell.execute_reply":"2025-02-17T02:47:28.549102Z"}},"outputs":[{"name":"stdout","text":"## The Enduring Significance of Olives in Modern Society\n\nThe olive, a seemingly humble fruit, holds a position of profound importance in modern society that extends far beyond its culinary applications. Its significance is woven into the fabric of history, culture, economy, and even health, demonstrating a resilience and adaptability that has secured its place at the table for millennia. While its impact might seem understated compared to flashier commodities, the olive's multifaceted contributions are undeniably substantial and deserving of detailed examination.\n\nHistorically, the olive tree has been a cornerstone of Mediterranean civilization.  Its cultivation predates recorded history, with evidence suggesting its domestication in the Levant thousands of years ago.  The olive's significance in ancient Greece and Rome is well-documented, representing not only a vital food source but also a symbol of peace, prosperity, and victory.  Olive oil fueled lamps, lubricated machinery, and served as a crucial element in religious ceremonies. The iconic olive branch, a symbol of peace, continues to\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:47:37.414418Z","iopub.execute_input":"2025-02-17T02:47:37.414749Z","iopub.status.idle":"2025-02-17T02:47:38.377775Z","shell.execute_reply.started":"2025-02-17T02:47:37.414726Z","shell.execute_reply":"2025-02-17T02:47:38.376971Z"}},"outputs":[{"name":"stdout","text":"From ancient groves, a humble fruit,\nThe olive thrives, a verdant shoot.\nIts oil, a gleam on countless plates,\nA flavour rich, that elevates.\n\nFrom salad bowls to lotions mild,\nIts bounty spreads, both pure and wild.\nA symbol strong, of sun-drenched lands,\nIn modern times, its value stands.\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"from google.api_core import retry\n\nhigh_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=2.0))\n\n\nretry_policy = {\n    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n}\n\nfor _ in range(5):\n  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                              request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:47:57.862317Z","iopub.execute_input":"2025-02-17T02:47:57.862650Z","iopub.status.idle":"2025-02-17T02:48:00.041534Z","shell.execute_reply.started":"2025-02-17T02:47:57.862625Z","shell.execute_reply":"2025-02-17T02:48:00.040128Z"}},"outputs":[{"name":"stdout","text":"Aquamarine\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMarigold\n -------------------------\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"low_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=0.0))\n\nfor _ in range(5):\n  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                             request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:48:11.270570Z","iopub.execute_input":"2025-02-17T02:48:11.270935Z","iopub.status.idle":"2025-02-17T02:48:13.505455Z","shell.execute_reply.started":"2025-02-17T02:48:11.270870Z","shell.execute_reply":"2025-02-17T02:48:13.504520Z"}},"outputs":[{"name":"stdout","text":"Maroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        # These are the default values for gemini-1.5-flash-001.\n        temperature=1.0,\n        top_k=64,\n        top_p=0.95,\n    ))\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = model.generate_content(story_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:48:22.900601Z","iopub.execute_input":"2025-02-17T02:48:22.900956Z","iopub.status.idle":"2025-02-17T02:48:25.958439Z","shell.execute_reply.started":"2025-02-17T02:48:22.900928Z","shell.execute_reply":"2025-02-17T02:48:25.957561Z"}},"outputs":[{"name":"stdout","text":"Bartholomew was a cat of routine. Every morning, he'd wake at precisely 6:03 AM, stretch, and demand breakfast. Every afternoon, he'd sun himself on the windowsill, batting at dust motes and dreaming of tuna. His life was as predictable as a sunrise.\n\nBut one Tuesday, something extraordinary happened. Bartholomew's human, a kindly old lady named Mildred, forgot to close the back door. A sudden urge, a primal call, drew him outside. The world was a symphony of smells - damp earth, blooming flowers, the musky scent of adventure. He took a tentative step, then another, and before he knew it, Bartholomew was bounding through the garden, a furry torpedo of curiosity.\n\nThe garden was a jungle to Bartholomew. He stalked a fat bumblebee, his tail twitching with excitement. He chased a sunbeam, its warmth a welcome embrace. He discovered a hidden creek, its sparkling waters a playground of splashing and leaping.\n\nHe ventured beyond the garden, his senses alight. The world was a kaleidoscope of sights and sounds. A fat squirrel chattered at him from a tree. A flock of pigeons took flight, their wings beating a frantic rhythm. He even encountered a dog, a fluffy white thing with a wagging tail and a wet nose that smelled of biscuits. They stared at each other, the dog cautiously, Bartholomew with a mixture of curiosity and apprehension. Then, the dog wagged its tail again, and Bartholomew felt a flicker of something akin to friendship.\n\nAs the sun began to dip below the horizon, a sense of weariness washed over Bartholomew. He missed Mildred, her warm lap and the comforting smell of her knitting. The world outside, while wondrous, was also vast and unknown. He missed the familiar scent of his own home.\n\nHe found his way back to the garden, then to the back door. Mildred was waiting for him, her face etched with worry. She scooped him up in her arms, whispering his name. As he snuggled into her embrace, he knew he was home.\n\nThe adventure was over, but Bartholomew was changed. The world had shown him its magic, its beauty, and its wildness. He still cherished his routines, but now, he also craved the occasional burst of the unknown. He learned that even a cat, a creature of routine and comfort, could find adventure, even in the most unexpected places. And sometimes, the greatest adventure of all is simply coming home. \n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=5,\n    ))\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:48:36.095554Z","iopub.execute_input":"2025-02-17T02:48:36.095923Z","iopub.status.idle":"2025-02-17T02:48:36.508528Z","shell.execute_reply.started":"2025-02-17T02:48:36.095871Z","shell.execute_reply":"2025-02-17T02:48:36.506959Z"}},"outputs":[{"name":"stdout","text":"Sentiment: **POSITIVE**\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ))\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:48:46.654190Z","iopub.execute_input":"2025-02-17T02:48:46.654488Z","iopub.status.idle":"2025-02-17T02:48:47.495667Z","shell.execute_reply.started":"2025-02-17T02:48:46.654460Z","shell.execute_reply":"2025-02-17T02:48:47.493987Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ))\n\nfew_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\n\nresponse = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:48:59.832918Z","iopub.execute_input":"2025-02-17T02:48:59.833238Z","iopub.status.idle":"2025-02-17T02:49:00.477170Z","shell.execute_reply.started":"2025-02-17T02:48:59.833212Z","shell.execute_reply":"2025-02-17T02:49:00.476282Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n  \"size\": \"large\",\n  \"type\": \"normal\",\n  \"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ))\n\nresponse = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:49:11.776697Z","iopub.execute_input":"2025-02-17T02:49:11.777073Z","iopub.status.idle":"2025-02-17T02:49:12.439132Z","shell.execute_reply.started":"2025-02-17T02:49:11.777045Z","shell.execute_reply":"2025-02-17T02:49:12.437364Z"}},"outputs":[{"name":"stdout","text":"{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert pizza\"}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content(prompt, request_options=retry_policy)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:49:21.243588Z","iopub.execute_input":"2025-02-17T02:49:21.243921Z","iopub.status.idle":"2025-02-17T02:49:21.705096Z","shell.execute_reply.started":"2025-02-17T02:49:21.243868Z","shell.execute_reply":"2025-02-17T02:49:21.703826Z"}},"outputs":[{"name":"stdout","text":"41\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = model.generate_content(prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:49:30.397959Z","iopub.execute_input":"2025-02-17T02:49:30.398243Z","iopub.status.idle":"2025-02-17T02:49:31.734537Z","shell.execute_reply.started":"2025-02-17T02:49:30.398220Z","shell.execute_reply":"2025-02-17T02:49:31.733632Z"}},"outputs":[{"name":"stdout","text":"Step 1: Find the partner's age when you were 4.\n\n* When you were 4, your partner was 3 times your age, so they were 3 * 4 = 12 years old.\n\nStep 2: Find the age difference between you and your partner.\n\n* The age difference is 12 - 4 = 8 years.\n\nStep 3: Find your partner's current age.\n\n* Since you are now 20 years old, and the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n\nTherefore, your partner is now $\\boxed{28}$ years old.\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:49:47.969184Z","iopub.execute_input":"2025-02-17T02:49:47.969513Z","iopub.status.idle":"2025-02-17T02:49:47.974817Z","shell.execute_reply.started":"2025-02-17T02:49:47.969488Z","shell.execute_reply":"2025-02-17T02:49:47.973842Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nreact_chat = model.start_chat()\n\n# You will perform the Action, so generate up to, but not including, the Observation.\nconfig = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n\nresp = react_chat.send_message(\n    [model_instructions, example1, example2, question],\n    generation_config=config,\n    request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:49:58.853969Z","iopub.execute_input":"2025-02-17T02:49:58.854274Z","iopub.status.idle":"2025-02-17T02:50:01.233381Z","shell.execute_reply.started":"2025-02-17T02:49:58.854250Z","shell.execute_reply":"2025-02-17T02:50:01.232279Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the Transformers NLP paper and then find the authors' ages to determine the youngest.  This will require searching for the paper and then likely examining the author list.  Finding ages will be difficult as it's not typically included in authorship information.\n\nAction 1\n<search>Transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:50:12.683963Z","iopub.execute_input":"2025-02-17T02:50:12.684324Z","iopub.status.idle":"2025-02-17T02:50:13.861220Z","shell.execute_reply.started":"2025-02-17T02:50:12.684299Z","shell.execute_reply":"2025-02-17T02:50:13.859971Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nThe observation gives me the authors of the paper \"Attention is All You Need\".  I cannot determine their ages from this information.  I need to find a way to get their ages or at least some biographical information.  A web search for each author might be necessary. This is inefficient.  There's no easy way to determine the youngest author from just the names.  I might need to look for information about the authors separately.\n\nAction 2\n<finish>I cannot answer this question. The provided text only lists the authors of the paper, not their ages.</finish>\n\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ))\n\n# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = model.generate_content(code_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:50:23.315521Z","iopub.execute_input":"2025-02-17T02:50:23.315835Z","iopub.status.idle":"2025-02-17T02:50:24.003010Z","shell.execute_reply.started":"2025-02-17T02:50:23.315806Z","shell.execute_reply":"2025-02-17T02:50:24.001827Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    tools='code_execution',)\n\ncode_exec_prompt = \"\"\"\nCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n\"\"\"\n\nresponse = model.generate_content(code_exec_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:50:34.077475Z","iopub.execute_input":"2025-02-17T02:50:34.077792Z","iopub.status.idle":"2025-02-17T02:50:37.278455Z","shell.execute_reply.started":"2025-02-17T02:50:34.077766Z","shell.execute_reply":"2025-02-17T02:50:37.277051Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"To calculate the sum of the first 14 odd prime numbers, I will first generate a list of prime numbers and then sum the first 14 odd primes from that list.\n\n\n``` python\ndef is_prime(n):\n    \"\"\"Checks if a number is prime.\"\"\"\n    if n <= 1:\n        return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n\nprimes = []\nnum = 2\ncount = 0\nwhile count < 14:\n    if is_prime(num) and num % 2 != 0:\n        primes.append(num)\n        count += 1\n    num += 1\n\nprint(f'{primes=}')\nsum_of_primes = sum(primes)\nprint(f'{sum_of_primes=}')\n\n\n```\n```\nprimes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes=326\n\n```\nThe first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47.  Their sum is 326.\n"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n  print(part)\n  print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:50:44.886433Z","iopub.execute_input":"2025-02-17T02:50:44.886752Z","iopub.status.idle":"2025-02-17T02:50:44.893457Z","shell.execute_reply.started":"2025-02-17T02:50:44.886730Z","shell.execute_reply":"2025-02-17T02:50:44.892317Z"}},"outputs":[{"name":"stdout","text":"text: \"To calculate the sum of the first 14 odd prime numbers, I will first generate a list of prime numbers and then sum the first 14 odd primes from that list.\\n\\n\"\n\n-----\nexecutable_code {\n  language: PYTHON\n  code: \"\\ndef is_prime(n):\\n    \\\"\\\"\\\"Checks if a number is prime.\\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\nprimes = []\\nnum = 2\\ncount = 0\\nwhile count < 14:\\n    if is_prime(num) and num % 2 != 0:\\n        primes.append(num)\\n        count += 1\\n    num += 1\\n\\nprint(f\\'{primes=}\\')\\nsum_of_primes = sum(primes)\\nprint(f\\'{sum_of_primes=}\\')\\n\\n\"\n}\n\n-----\ncode_execution_result {\n  outcome: OUTCOME_OK\n  output: \"primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nsum_of_primes=326\\n\"\n}\n\n-----\ntext: \"The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47.  Their sum is 326.\\n\"\n\n-----\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nresponse = model.generate_content(explain_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T02:50:55.590377Z","iopub.execute_input":"2025-02-17T02:50:55.590679Z","iopub.status.idle":"2025-02-17T02:50:58.574620Z","shell.execute_reply.started":"2025-02-17T02:50:55.590653Z","shell.execute_reply":"2025-02-17T02:50:58.573439Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a bash script that enhances your shell prompt to display information about your current Git repository.  Think of it as a highly customizable Git status indicator for your terminal.\n\n**What it does at a high level:**\n\nThe script adds to your shell prompt (the line you type commands on) information like:\n\n* **Current Git branch:**  e.g., `main`, `develop`\n* **Ahead/Behind status:** Shows how many commits you're ahead of or behind the remote repository.\n* **Uncommitted changes:** Indicates if you have uncommitted changes in your working directory or staging area.\n* **Status symbols:** Uses customizable symbols to represent the different statuses (e.g., * for ahead, - for behind).\n* **Theme support:** Allows you to customize the colors and appearance of the prompt using different themes.\n\n**Why you would use it:**\n\nThis script improves your workflow by providing a concise and visually informative Git status directly within your terminal.  You don't need to constantly run `git status` to check the state of your repository.  It's particularly helpful for developers working on multiple Git projects simultaneously.\n\n**In short:** It makes your shell prompt \"smarter\" by integrating useful Git information, improving your development experience.\n"},"metadata":{}}],"execution_count":40}]}